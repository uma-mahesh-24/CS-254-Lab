{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uma-mahesh-24/CS-254-Lab/blob/main/segformer_hf_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4x07w994VdY"
      },
      "source": [
        "# %% [imports]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSVe3AXl3ZK2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import (\n",
        "    SegformerConfig,\n",
        "    SegformerForSemanticSegmentation,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "from transformers.models.segformer.modeling_segformer import SemanticSegmenterOutput\n",
        "\n",
        "# NEW: Hugging Face Datasets for streaming / memory-mapped data\n",
        "from datasets import Dataset, DatasetDict, load_dataset, Features, Array2D, Value, IterableDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkLfwl5o4Tg2"
      },
      "source": [
        "# %% [mount drive]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWuCQlal3jsb",
        "outputId": "26e9f076-4d97-4a5e-d724-29eec661564a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Data root: /content/drive/MyDrive/corrected_dataset\n",
            "Output dir: /content/drive/MyDrive/train-xii-weighted-sse\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# ==== PATHS (edit if needed) ====\n",
        "root_dir = \"/content/drive/MyDrive\"  # Base folder in Drive\n",
        "data_dir = os.path.join(root_dir, \"corrected_dataset\")\n",
        "\n",
        "inputs_dir = os.path.join(data_dir, \"inputs\")\n",
        "targets_dir = os.path.join(data_dir, \"targets\")\n",
        "weights_dir = os.path.join(data_dir, \"weights30_drastic\")\n",
        "\n",
        "output_dir = os.path.join(root_dir, \"train-xii-weighted-sse\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Data root: {data_dir}\")\n",
        "print(f\"Output dir: {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahDrajaG4Rq_"
      },
      "source": [
        "# %% [dataset, new with HF datasets module + random flips]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv7OXrpx3lur"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "# NEW: Hugging Face Datasets for streaming / memory-mapped data\n",
        "from datasets import Dataset, DatasetDict, load_dataset, Features, Array2D, Value, IterableDataset\n",
        "\n",
        "# Define the data augmentation functions\n",
        "def random_horizontal_flip(x, y, w):\n",
        "    \"\"\"Applies a random horizontal flip to the input, label, and weights.\"\"\"\n",
        "    if np.random.rand() < 0.5:\n",
        "        return np.fliplr(x).copy(), np.fliplr(y).copy(), np.fliplr(w).copy() # Add .copy()\n",
        "    return x, y, w\n",
        "\n",
        "def random_vertical_flip(x, y, w):\n",
        "    \"\"\"Applies a random vertical flip to the input, label, and weights.\"\"\"\n",
        "    if np.random.rand() < 0.5:\n",
        "        return np.flipud(x).copy(), np.flipud(y).copy(), np.flipud(w).copy() # Add .copy()\n",
        "    return x, y, w\n",
        "\n",
        "def npy_generator_augmented(file_list: List[str], inputs_dir: str, targets_dir: str, weights_dir: str,\n",
        "                            standardize_targets: bool=False, target_mean: Optional[float]=None,\n",
        "                            target_std: Optional[float]=None):\n",
        "    \"\"\"Yields one augmented example at a time, keeping memory usage low.\"\"\"\n",
        "    for fname in file_list:\n",
        "        # Load the data\n",
        "        x = np.load(os.path.join(inputs_dir, fname)).astype(np.float32)\n",
        "        y = np.load(os.path.join(targets_dir, fname)).astype(np.float32)\n",
        "        w = np.load(os.path.join(weights_dir, fname)).astype(np.float32)\n",
        "\n",
        "        # Apply augmentations (flips are dimension-preserving)\n",
        "        x, y, w = random_horizontal_flip(x, y, w)\n",
        "        x, y, w = random_vertical_flip(x, y, w)\n",
        "\n",
        "        # Standardize targets if enabled\n",
        "        if standardize_targets and target_mean is not None and target_std is not None:\n",
        "            y = (y - target_mean) / (target_std + 1e-8)\n",
        "\n",
        "        yield {\n",
        "            \"file_name\": fname,\n",
        "            \"pixel_values\": x[None, :, :],\n",
        "            \"labels\": y[None, :, :],\n",
        "            \"weights\": w[None, :, :],\n",
        "        }\n",
        "\n",
        "def npy_generator(file_list: List[str], inputs_dir: str, targets_dir: str, weights_dir: str,\n",
        "                            standardize_targets: bool=False, target_mean: Optional[float]=None,\n",
        "                            target_std: Optional[float]=None):\n",
        "    \"\"\"Yields one unaugmented example at a time, keeping memory usage low.\"\"\"\n",
        "    for fname in file_list:\n",
        "        # Load the data\n",
        "        x = np.load(os.path.join(inputs_dir, fname)).astype(np.float32)\n",
        "        y = np.load(os.path.join(targets_dir, fname)).astype(np.float32)\n",
        "        w = np.load(os.path.join(weights_dir, fname)).astype(np.float32)\n",
        "\n",
        "        # Standardize targets if enabled\n",
        "        if standardize_targets and target_mean is not None and target_std is not None:\n",
        "            y = (y - target_mean) / (target_std + 1e-8)\n",
        "\n",
        "        yield {\n",
        "            \"file_name\": fname,\n",
        "            \"pixel_values\": x[None, :, :],\n",
        "            \"labels\": y[None, :, :],\n",
        "            \"weights\": w[None, :, :],\n",
        "        }\n",
        "\n",
        "\n",
        "def create_hf_datasets(inputs_dir: str, targets_dir: str, weights_dir: str,\n",
        "                       standardize_targets: bool=False, target_mean: Optional[float]=None,\n",
        "                       target_std: Optional[float]=None) -> DatasetDict:\n",
        "    # Collect file list\n",
        "    all_files = sorted([os.path.basename(p) for p in glob.glob(os.path.join(inputs_dir, \"*.npy\"))])\n",
        "    split_idx = int(0.9 * len(all_files))\n",
        "    train_files = all_files[:split_idx]\n",
        "    val_files = all_files[split_idx:]\n",
        "\n",
        "    # Use IterableDataset.from_generator with the augmented generator\n",
        "    train_ds = IterableDataset.from_generator(\n",
        "        npy_generator_augmented,\n",
        "        gen_kwargs=dict(\n",
        "            file_list=train_files,\n",
        "            inputs_dir=inputs_dir,\n",
        "            targets_dir=targets_dir,\n",
        "            weights_dir=weights_dir,\n",
        "            standardize_targets=standardize_targets,\n",
        "            target_mean=target_mean,\n",
        "            target_std=target_std,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Use a separate, unaugmented generator for the validation set\n",
        "    val_ds = IterableDataset.from_generator(\n",
        "        npy_generator,\n",
        "        gen_kwargs=dict(\n",
        "            file_list=val_files,\n",
        "            inputs_dir=inputs_dir,\n",
        "            targets_dir=targets_dir,\n",
        "            weights_dir=weights_dir,\n",
        "            standardize_targets=standardize_targets,\n",
        "            target_mean=target_mean,\n",
        "            target_std=target_std,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return DatasetDict({\"train\": train_ds, \"validation\": val_ds})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c_6Z9oa4PWe"
      },
      "source": [
        "# %% [split]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7JwpfGY3oKM",
        "outputId": "3bdcb51b-d748-4ea1-97ec-9a490bc7114e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HF IterableDataset created (streaming). Train and validation will be loaded on-the-fly.\n"
          ]
        }
      ],
      "source": [
        "use_standardization = False\n",
        "t_mean, t_std = 0.0, 1.0\n",
        "\n",
        "if use_standardization:\n",
        "    stats_path = os.path.join(targets_dir, \"standardization_vals.json\")\n",
        "    if not os.path.exists(stats_path):\n",
        "        # compute stats only once on training files\n",
        "        all_files = sorted([f for f in os.listdir(inputs_dir) if f.endswith(\".npy\")])\n",
        "        split_idx = int(0.9 * len(all_files))\n",
        "        train_files = all_files[:split_idx]\n",
        "\n",
        "        sample_vals = []\n",
        "        for fname in train_files:\n",
        "            y = np.load(os.path.join(targets_dir, fname)).astype(np.float32)\n",
        "            valid = y[y >= 0]\n",
        "            sample_vals.append(valid)\n",
        "        sample_vals = np.concatenate(sample_vals)\n",
        "        t_mean = float(sample_vals.mean())\n",
        "        t_std  = float(sample_vals.std() + 1e-8)\n",
        "\n",
        "        with open(stats_path, \"w\") as f:\n",
        "            json.dump({\"t_mean\": t_mean, \"t_std\": t_std}, f, indent=4)\n",
        "\n",
        "    with open(stats_path, \"r\") as f:\n",
        "        stats = json.load(f)\n",
        "    t_mean, t_std = stats[\"t_mean\"], stats[\"t_std\"]\n",
        "\n",
        "hf_datasets = create_hf_datasets(\n",
        "    inputs_dir=inputs_dir,\n",
        "    targets_dir=targets_dir,\n",
        "    weights_dir=weights_dir,\n",
        "    standardize_targets=use_standardization,\n",
        "    target_mean=t_mean,\n",
        "    target_std=t_std,\n",
        ")\n",
        "\n",
        "train_ds = hf_datasets[\"train\"]\n",
        "val_ds = hf_datasets[\"validation\"]\n",
        "\n",
        "print(\"HF IterableDataset created (streaming). Train and validation will be loaded on-the-fly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rofjvK7qQc-5"
      },
      "source": [
        "# %% [custom loss functions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_i_a48lQcqg"
      },
      "outputs": [],
      "source": [
        "def weighted_sse_loss(pred, target, weights):\n",
        "    diff = (pred - target) ** 2\n",
        "    diff = diff * weights  # simpler and avoids creating a new tensor via torch.mul\n",
        "\n",
        "    if diff.numel() == 0:\n",
        "        return torch.tensor(0.0, device=pred.device)\n",
        "    return diff.sum()  # still sum â€” it's SSE\n",
        "\n",
        "def weighted_mse_loss(pred, target, weights):\n",
        "    diff = (pred - target) ** 2\n",
        "    diff = diff * weights  # simpler and avoids creating a new tensor via torch.mul\n",
        "    mask = target >= 0\n",
        "\n",
        "    if diff.numel() == 0:\n",
        "        return torch.tensor(0.0, device=pred.device)\n",
        "    return diff.sum() / mask.sum()  # taking mean\n",
        "\n",
        "def masked_sse_loss_no_weights(pred, target, weights):\n",
        "    \"\"\"\n",
        "    Computes SSE loss using a binary mask derived from the target, ignoring weights.\n",
        "    Assumes target >= 0 is the valid mask.\n",
        "    \"\"\"\n",
        "    mask = target >= 0\n",
        "    diff = (pred - target) ** 2\n",
        "    masked_diff = diff[mask]  # Apply the mask\n",
        "\n",
        "    if masked_diff.numel() == 0:\n",
        "        return torch.tensor(0.0, device=pred.device)\n",
        "    return masked_diff.sum()  # Sum the masked squared differences\n",
        "\n",
        "def masked_mse_loss(pred, target):\n",
        "    mask = target >= 0\n",
        "    diff = (pred - target) ** 2\n",
        "    diff = diff * mask  # boolean mask auto-broadcasts\n",
        "\n",
        "    if mask.sum() == 0:\n",
        "        return torch.tensor(0.0, device=pred.device)\n",
        "\n",
        "    return diff.sum() / mask.sum()  # use *mean* to make it comparable across batch sizes\n",
        "\n",
        "def masked_mean_abs_loss(pred, target):\n",
        "    mask = target >= 0\n",
        "    diff = torch.abs(pred - target)\n",
        "\n",
        "    if mask.sum() == 0:\n",
        "        return torch.tensor(0.0, device=pred.device)\n",
        "\n",
        "    return diff[mask].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g1KCqZx4NFn"
      },
      "source": [
        "# %% [model]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XttjdZGE3qg8",
        "outputId": "f9d0e390-273d-443a-a3cc-6513cc4f2c5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForPixelRegression were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
            "- segformer.encoder.patch_embeddings.0.proj.weight: found shape torch.Size([32, 3, 7, 7]) in the checkpoint and torch.Size([32, 1, 7, 7]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "class SegformerForPixelRegression(SegformerForSemanticSegmentation):\n",
        "    \"\"\"\n",
        "    Wrap SegformerForSemanticSegmentation but treat it as a per-pixel regression model:\n",
        "    - num_labels=1\n",
        "    - loss = MSE between logits and labels (both shape: [B,1,H,W])\n",
        "    \"\"\"\n",
        "    def forward(self, pixel_values: torch.FloatTensor, labels: Optional[torch.FloatTensor] = None, weights: Optional[torch.FloatTensor] = None, **kwargs):\n",
        "        valid_keys = [\"output_attentions\", \"output_hidden_states\", \"return_dict\"]\n",
        "        filtered_kwargs = {k: v for k, v in kwargs.items() if k in valid_keys}\n",
        "\n",
        "        outputs = super().forward(pixel_values=pixel_values, labels=None, **filtered_kwargs)\n",
        "        logits = outputs.logits  # [B,1,H/4,W/4]\n",
        "\n",
        "        # ðŸ”§ resize logits back to input size (e.g., HxW)\n",
        "        logits = F.interpolate(logits, size=pixel_values.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None and weights is not None:\n",
        "            if labels.ndim == 3:\n",
        "                labels = labels.unsqueeze(1)\n",
        "            if weights.ndim == 3:\n",
        "                weights = weights.unsqueeze(1)\n",
        "            # loss = F.mse_loss(logits, labels)\n",
        "            # loss = weighted_sse_loss(logits, labels, weights)\n",
        "            # loss = weighted_mse_loss(logits, labels, weights)\n",
        "            loss = masked_sse_loss_no_weights(logits, labels, weights)\n",
        "\n",
        "        return SemanticSegmenterOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "# Start with a small SegFormer; you can scale to b1/b2/b3 etc.\n",
        "base_ckpt = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
        "\n",
        "# configure single-channel input + single-channel output\n",
        "cfg = SegformerConfig.from_pretrained(base_ckpt)\n",
        "cfg.num_channels = 1          # single-channel inputs\n",
        "cfg.num_labels = 1            # one regression map\n",
        "cfg.ignore_mismatched_sizes = True\n",
        "\n",
        "# load pretrained (encoder/decoder) weights where sizes match\n",
        "model = SegformerForPixelRegression.from_pretrained(\n",
        "    base_ckpt,\n",
        "    config=cfg,\n",
        "    ignore_mismatched_sizes=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBDoLTH94Kjv"
      },
      "source": [
        "# %% [trainer utils]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i__xWneU3tgN"
      },
      "outputs": [],
      "source": [
        "def data_collator(batch):\n",
        "    # Since all images are same size (512x512), no need for dynamic padding.\n",
        "    return {\n",
        "        \"pixel_values\": torch.stack([torch.from_numpy(b[\"pixel_values\"]) for b in batch]),  # [B,1,H,W]\n",
        "        \"labels\": torch.stack([torch.from_numpy(b[\"labels\"]) for b in batch]),              # [B,1,H,W]\n",
        "        \"weights\": torch.stack([torch.from_numpy(b[\"weights\"]) for b in batch]),            # [B,1,H,W]\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "\n",
        "    # Convert to numpy if tensors (HF sometimes gives np.ndarray, sometimes torch.Tensor)\n",
        "    if isinstance(preds, torch.Tensor):\n",
        "        preds = preds.cpu().numpy()\n",
        "    if isinstance(labels, torch.Tensor):\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "    # Ensure shapes: [B,1,H,W] -> [B,H,W]\n",
        "    if preds.ndim == 4 and preds.shape[1] == 1:\n",
        "        preds = preds[:, 0, :, :]\n",
        "    if labels.ndim == 4 and labels.shape[1] == 1:\n",
        "        labels = labels[:, 0, :, :]\n",
        "\n",
        "    # Mask invalid labels\n",
        "    mask = labels >= 0\n",
        "    preds = preds[mask]\n",
        "    labels = labels[mask]\n",
        "\n",
        "    # Avoid division by zero in empty mask case\n",
        "    if preds.size == 0:\n",
        "        return {\"mae\": np.nan, \"rmse\": np.nan, \"r2\": np.nan}\n",
        "\n",
        "    # De-standardize if enabled\n",
        "    if use_standardization:\n",
        "        preds = preds * (t_std + 1e-8) + t_mean\n",
        "        labels = labels * (t_std + 1e-8) + t_mean\n",
        "\n",
        "    mae = float(np.mean(np.abs(preds - labels)))\n",
        "    rmse = float(np.sqrt(np.mean((preds - labels) ** 2)))\n",
        "\n",
        "    # r2_score can crash on constant labels (avoid that)\n",
        "    try:\n",
        "        r2 = float(r2_score(labels, preds))\n",
        "    except ValueError:\n",
        "        r2 = float(\"nan\")\n",
        "\n",
        "    return {\"mae\": mae, \"rmse\": rmse, \"r2\": r2}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6Za1XT34H5V"
      },
      "source": [
        "# %% [training args]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJzwbDki3vQV"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    learning_rate=5e-4,\n",
        "    weight_decay=0.01,\n",
        "    # num_train_epochs=50,              # keep as-is, single continuous run\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.05,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",      # <- use correct HF arg name\n",
        "    eval_steps=200,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,               # keep disk usage low\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"rmse\",\n",
        "    greater_is_better=False,\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=2,\n",
        "    report_to=\"none\",\n",
        "    dataloader_pin_memory=True,       # slight speed boost\n",
        "    save_safetensors=True,            # safer + faster checkpoint format\n",
        "    max_steps=39400 + 7000,\n",
        "    ignore_data_skip=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_ds,   # (replace with HF Dataset object later)\n",
        "    eval_dataset=val_ds,      # (replace with HF Dataset object later)\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OotPe_GK4FgO"
      },
      "source": [
        "# %% [train]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n1Azzjdf_Y-3",
        "outputId": "4d3eb716-2122-45b8-b478-24652c82ea71"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='45801' max='46400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [45801/46400 24:35 < 02:18, 4.34 it/s, Epoch 8.00/9223372036854775807]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Mae</th>\n",
              "      <th>Rmse</th>\n",
              "      <th>R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>39600</td>\n",
              "      <td>17091.101200</td>\n",
              "      <td>7625.028320</td>\n",
              "      <td>0.053171</td>\n",
              "      <td>0.095347</td>\n",
              "      <td>0.879943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39800</td>\n",
              "      <td>16205.760000</td>\n",
              "      <td>7294.237305</td>\n",
              "      <td>0.051199</td>\n",
              "      <td>0.093255</td>\n",
              "      <td>0.885152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40000</td>\n",
              "      <td>28686.630000</td>\n",
              "      <td>7914.490234</td>\n",
              "      <td>0.053276</td>\n",
              "      <td>0.097141</td>\n",
              "      <td>0.875383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40200</td>\n",
              "      <td>13614.930000</td>\n",
              "      <td>7346.259277</td>\n",
              "      <td>0.050334</td>\n",
              "      <td>0.093587</td>\n",
              "      <td>0.884334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40400</td>\n",
              "      <td>15589.186300</td>\n",
              "      <td>7302.291992</td>\n",
              "      <td>0.049919</td>\n",
              "      <td>0.093308</td>\n",
              "      <td>0.885022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40600</td>\n",
              "      <td>14734.046200</td>\n",
              "      <td>6932.196777</td>\n",
              "      <td>0.048791</td>\n",
              "      <td>0.090914</td>\n",
              "      <td>0.890848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40800</td>\n",
              "      <td>26250.007500</td>\n",
              "      <td>7412.987305</td>\n",
              "      <td>0.050371</td>\n",
              "      <td>0.094011</td>\n",
              "      <td>0.883284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41000</td>\n",
              "      <td>14423.162500</td>\n",
              "      <td>6965.774902</td>\n",
              "      <td>0.048372</td>\n",
              "      <td>0.091132</td>\n",
              "      <td>0.890324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41200</td>\n",
              "      <td>13798.075000</td>\n",
              "      <td>6911.824219</td>\n",
              "      <td>0.048373</td>\n",
              "      <td>0.090779</td>\n",
              "      <td>0.891172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41400</td>\n",
              "      <td>14357.516300</td>\n",
              "      <td>6782.726562</td>\n",
              "      <td>0.047937</td>\n",
              "      <td>0.089926</td>\n",
              "      <td>0.893207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41600</td>\n",
              "      <td>25638.085000</td>\n",
              "      <td>7177.770508</td>\n",
              "      <td>0.049279</td>\n",
              "      <td>0.092507</td>\n",
              "      <td>0.886988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41800</td>\n",
              "      <td>12972.703800</td>\n",
              "      <td>6970.373047</td>\n",
              "      <td>0.048129</td>\n",
              "      <td>0.091162</td>\n",
              "      <td>0.890250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42000</td>\n",
              "      <td>14892.902500</td>\n",
              "      <td>6839.326172</td>\n",
              "      <td>0.047943</td>\n",
              "      <td>0.090302</td>\n",
              "      <td>0.892312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42200</td>\n",
              "      <td>16937.532500</td>\n",
              "      <td>6754.211426</td>\n",
              "      <td>0.048267</td>\n",
              "      <td>0.089738</td>\n",
              "      <td>0.893654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42400</td>\n",
              "      <td>27291.072500</td>\n",
              "      <td>6919.210938</td>\n",
              "      <td>0.048439</td>\n",
              "      <td>0.090826</td>\n",
              "      <td>0.891058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42600</td>\n",
              "      <td>13962.765000</td>\n",
              "      <td>6791.797852</td>\n",
              "      <td>0.047709</td>\n",
              "      <td>0.089987</td>\n",
              "      <td>0.893061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42800</td>\n",
              "      <td>16795.948700</td>\n",
              "      <td>6809.687988</td>\n",
              "      <td>0.047969</td>\n",
              "      <td>0.090106</td>\n",
              "      <td>0.892778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43000</td>\n",
              "      <td>19741.321300</td>\n",
              "      <td>6704.619141</td>\n",
              "      <td>0.047614</td>\n",
              "      <td>0.089408</td>\n",
              "      <td>0.894434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43200</td>\n",
              "      <td>27764.107500</td>\n",
              "      <td>6808.844727</td>\n",
              "      <td>0.047787</td>\n",
              "      <td>0.090099</td>\n",
              "      <td>0.892794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43400</td>\n",
              "      <td>13890.661200</td>\n",
              "      <td>6680.330566</td>\n",
              "      <td>0.046905</td>\n",
              "      <td>0.089245</td>\n",
              "      <td>0.894818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43600</td>\n",
              "      <td>16387.986300</td>\n",
              "      <td>6768.605469</td>\n",
              "      <td>0.047510</td>\n",
              "      <td>0.089834</td>\n",
              "      <td>0.893425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43800</td>\n",
              "      <td>22941.320000</td>\n",
              "      <td>6698.041504</td>\n",
              "      <td>0.047152</td>\n",
              "      <td>0.089363</td>\n",
              "      <td>0.894539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44000</td>\n",
              "      <td>27911.522500</td>\n",
              "      <td>6814.467285</td>\n",
              "      <td>0.047621</td>\n",
              "      <td>0.090137</td>\n",
              "      <td>0.892705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44200</td>\n",
              "      <td>12923.440000</td>\n",
              "      <td>6659.532227</td>\n",
              "      <td>0.046651</td>\n",
              "      <td>0.089107</td>\n",
              "      <td>0.895144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44400</td>\n",
              "      <td>17434.278800</td>\n",
              "      <td>6699.108398</td>\n",
              "      <td>0.046987</td>\n",
              "      <td>0.089371</td>\n",
              "      <td>0.894520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44600</td>\n",
              "      <td>24547.680000</td>\n",
              "      <td>6688.564941</td>\n",
              "      <td>0.047393</td>\n",
              "      <td>0.089300</td>\n",
              "      <td>0.894688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44800</td>\n",
              "      <td>26045.502500</td>\n",
              "      <td>6689.591309</td>\n",
              "      <td>0.047024</td>\n",
              "      <td>0.089307</td>\n",
              "      <td>0.894672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45000</td>\n",
              "      <td>13442.768700</td>\n",
              "      <td>6672.880371</td>\n",
              "      <td>0.046722</td>\n",
              "      <td>0.089195</td>\n",
              "      <td>0.894935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45200</td>\n",
              "      <td>15500.205000</td>\n",
              "      <td>6675.586914</td>\n",
              "      <td>0.046853</td>\n",
              "      <td>0.089214</td>\n",
              "      <td>0.894891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45400</td>\n",
              "      <td>25158.725000</td>\n",
              "      <td>6670.198242</td>\n",
              "      <td>0.047382</td>\n",
              "      <td>0.089177</td>\n",
              "      <td>0.894978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45600</td>\n",
              "      <td>25525.027500</td>\n",
              "      <td>6641.710938</td>\n",
              "      <td>0.046968</td>\n",
              "      <td>0.088986</td>\n",
              "      <td>0.895427</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "try:\n",
        "  trainer.train(resume_from_checkpoint=True)\n",
        "except Exception as e:\n",
        "  print(f\"Exception occurred: {e}\")\n",
        "  print(\"Starting training from scratch\")\n",
        "  trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Dca9YT64DRA"
      },
      "source": [
        "# %% [eval]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEQ35aBA3yPF"
      },
      "outputs": [],
      "source": [
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NBDoLTH94Kjv"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}